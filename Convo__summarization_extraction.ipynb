{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachitrajeshParihar/Convo__summarization_extraction/blob/main/Convo__summarization_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sQZwQx9htQFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faA2URQyRVOv"
      },
      "source": [
        "# Conversation Summarization & Structured Extraction with Groq API\n",
        "\n",
        "## This notebook demonstrates how to:\n",
        "\n",
        "1.   Summarize user ↔ assistant conversations.\n",
        "2.   Extract structured JSON data (intent, topics, sentiment, named entities, action items, confidence score) from conversations.\n",
        "\n",
        "\n",
        "We will use Groq APIs, which are OpenAI-SDK compatible.\n",
        "That means you can call them using openai-style code (client.chat.completions.create) or via raw HTTP requests with requests.\n",
        "\n",
        "## How to use this notebook :\n",
        "1. Run dependency installation.\n",
        "2. Enter your Groq API key (either via Colab prompt or environment variable).\n",
        "3. Test summarization and structured extraction on sample conversations.\n",
        "4. Batch process and export results to .jsonl for downstream analysis.\n",
        "5. Save outputs (README.md, LICENSE, requirements file) for publishing to GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "* Use the grok API which has access to openAI's supproted models. This notebook does not provide any such API.\n"
      ],
      "metadata": {
        "id": "wi8_IbhXpxy1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbGdXAJ-RWWe"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "!pip install -q openai pandas requests python-dotenv tqdm\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import logging\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "except ImportError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JchgTCLKTySL"
      },
      "source": [
        "##Authentication\n",
        "You can authenticate in two ways:\n",
        "\n",
        "Option A: Enter API key interactively (not saved).\n",
        "\n",
        "Option B: Store API key in **.env** or environment variables (**GROQ_API_KEY**).\n",
        "\n",
        "In Google Colab, you can safely store secrets via:\n",
        "\n",
        "**%env GROQ_API_KEY = your_key_here**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUtQBGsxURzE"
      },
      "outputs": [],
      "source": [
        "# Load environment variables (if .env exists)\n",
        "try:\n",
        "    load_dotenv()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Option A: Interactive prompt\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "if not GROQ_API_KEY:\n",
        "    GROQ_API_KEY = input(\"Enter GROQ API key (won't be saved): \").strip()\n",
        "\n",
        "assert GROQ_API_KEY, \"Groq API key must be provided\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "* The open AI model can be changed in the code as per convenience. You need to changed the commented part in the following cell."
      ],
      "metadata": {
        "id": "Am_xaqLmq9Dn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtYkf2NyUjFv"
      },
      "outputs": [],
      "source": [
        "class GroqClientWrapper:\n",
        "    \"\"\"\n",
        "    Wrapper to interact with Groq API, compatible with OpenAI SDK style.\n",
        "    Provides fallback to raw HTTP requests if needed.\n",
        "    \"\"\"\n",
        "    def __init__(self, api_key: str, base_url: Optional[str] = None, use_openai_client: bool = True):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = base_url or \"https://api.groq.com/openai/v1\"\n",
        "        self.use_openai_client = use_openai_client\n",
        "\n",
        "        # Setup logging\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "        self.logger = logging.getLogger(\"GroqClientWrapper\")\n",
        "\n",
        "        if use_openai_client:\n",
        "            try:\n",
        "                import openai\n",
        "                self.client = openai.OpenAI(api_key=api_key, base_url=self.base_url)\n",
        "            except Exception as e:\n",
        "                self.logger.warning(\"OpenAI client not available, falling back to requests.\")\n",
        "                self.use_openai_client = False\n",
        "                self.client = None\n",
        "        else:\n",
        "            self.client = None\n",
        "\n",
        "    def chat_completion(self, messages: List[Dict[str, str]], model: str = \"openai/gpt-oss-120b\", # Change model here : model: str = \"YOUR_MODEL_NAME\"\n",
        "                        temperature: float = 0.0, max_tokens: int = 512, **kwargs) -> str:\n",
        "        \"\"\"Perform chat completion with retries and exponential backoff.\"\"\"\n",
        "        retries = 5\n",
        "        backoff = 1\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                if self.use_openai_client and self.client:\n",
        "                    resp = self.client.chat.completions.create(\n",
        "                        model=model,\n",
        "                        messages=messages,\n",
        "                        temperature=temperature,\n",
        "                        max_tokens=max_tokens,\n",
        "                        **kwargs\n",
        "                    )\n",
        "                    return resp.choices[0].message.content.strip()\n",
        "                else:\n",
        "                    headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
        "                    url = f\"{self.base_url}/chat/completions\"\n",
        "                    payload = {\n",
        "                        \"model\": model,\n",
        "                        \"messages\": messages,\n",
        "                        \"temperature\": temperature,\n",
        "                        \"max_tokens\": max_tokens,\n",
        "                    }\n",
        "                    r = requests.post(url, headers=headers, json=payload)\n",
        "                    if r.status_code == 200:\n",
        "                        js = r.json()\n",
        "                        return js[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "                    elif r.status_code == 429:\n",
        "                        self.logger.warning(\"Rate limit hit, retrying...\")\n",
        "                        time.sleep(backoff)\n",
        "                        backoff *= 2\n",
        "                        continue\n",
        "                    else:\n",
        "                        r.raise_for_status()\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Error: {e}, retrying in {backoff}s...\")\n",
        "                time.sleep(backoff)\n",
        "                backoff *= 2\n",
        "\n",
        "        raise RuntimeError(\"Max retries exceeded for chat_completion\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a0CVz7OUpG7"
      },
      "outputs": [],
      "source": [
        "# Prompt templates\n",
        "\n",
        "SUMMARIZATION_SYSTEM_PROMPT = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"You are a helpful assistant that summarizes conversations in 1-3 concise sentences.\"\n",
        "}\n",
        "\n",
        "STRUCTURED_EXTRACTION_SYSTEM_PROMPT = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"You are an information extraction assistant.\n",
        "Extract structured data as a strict JSON object with the following schema:\n",
        "{\n",
        "  \"conversation_id\": \"<string>\",\n",
        "  \"summary\": \"<string>\",\n",
        "  \"intent\": \"<string>\",\n",
        "  \"topics\": [\"<string>\", ...],\n",
        "  \"sentiment\": \"positive|neutral|negative\",\n",
        "  \"named_entities\": [\"<string>\", ...],\n",
        "  \"action_items\": [\"<string>\", ...],\n",
        "  \"confidence\": <float 0.0-1.0>,\n",
        "  \"schema_version\": \"1.0\"\n",
        "}\n",
        "Output ONLY a single valid JSON object (no markdown, no commentary).\n",
        "If you cannot extract a value leave empty string or empty array.\n",
        "Confidence should be numeric 0-1.\n",
        "\"\"\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6C8AY9XUrXb"
      },
      "outputs": [],
      "source": [
        "def load_sample_conversations() -> pd.DataFrame:\n",
        "    \"\"\"Return small dataset of conversations for demonstration.\"\"\"\n",
        "    sample_data = [\n",
        "        {\n",
        "            \"conversation_id\": \"conv1\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": \"Can you suggest resources to learn Python?\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"Sure! You can try Codecademy, Real Python, or the official docs.\"}\n",
        "            ],\n",
        "            \"annotated_intent\": \"learn_resources\"\n",
        "        },\n",
        "        {\n",
        "            \"conversation_id\": \"conv2\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": \"Your app keeps crashing when I click 'Export'.\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"Sorry to hear! Could you provide the error message?\"}\n",
        "            ],\n",
        "            \"annotated_intent\": \"report_bug\"\n",
        "        },\n",
        "        {\n",
        "            \"conversation_id\": \"conv3\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": \"Can I get a copy of my invoice for last month?\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"Yes, you can download it in your billing dashboard.\"}\n",
        "            ],\n",
        "            \"annotated_intent\": \"billing_query\"\n",
        "        },\n",
        "        {\n",
        "            \"conversation_id\": \"conv4\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": \"Please cancel my subscription effective immediately.\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"Understood. I will process your cancellation.\"}\n",
        "            ],\n",
        "            \"annotated_intent\": \"cancel_subscription\"\n",
        "        },\n",
        "    ]\n",
        "    return pd.DataFrame(sample_data)\n",
        "\n",
        "def format_messages_for_api(messages: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
        "    return messages\n",
        "\n",
        "def safe_parse_json(text: str) -> Tuple[Optional[Dict[str, Any]], bool]:\n",
        "    \"\"\"Attempt to safely parse JSON from text. Handle common formatting issues.\"\"\"\n",
        "    try:\n",
        "        return json.loads(text), True\n",
        "    except Exception:\n",
        "        match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
        "        if match:\n",
        "            candidate = match.group(0)\n",
        "            candidate = re.sub(r\",\\s*}\", \"}\", candidate)\n",
        "            candidate = re.sub(r\",\\s*]\", \"]\", candidate)\n",
        "            try:\n",
        "                return json.loads(candidate), True\n",
        "            except Exception:\n",
        "                return None, False\n",
        "    return None, False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "3aJB1UlFUt5K",
        "outputId": "0f902465-1520-4e10-a951-def2ab7ef0e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:GroqClientWrapper:Error: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}, retrying in 1s...\n",
            "WARNING:GroqClientWrapper:Error: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}, retrying in 2s...\n",
            "WARNING:GroqClientWrapper:Error: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}, retrying in 4s...\n",
            "WARNING:GroqClientWrapper:Error: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}, retrying in 8s...\n",
            "WARNING:GroqClientWrapper:Error: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}, retrying in 16s...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Max retries exceeded for chat_completion",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1206981642.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_sample_conversations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroqClientWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGROQ_API_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_conversation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconversation_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1206981642.py\u001b[0m in \u001b[0;36msummarize_conversation\u001b[0;34m(conversation_id, messages, client, max_tokens)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSUMMARIZATION_SYSTEM_PROMPT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mformat_messages_for_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     return {\n",
            "\u001b[0;32m/tmp/ipython-input-2138646363.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, temperature, max_tokens, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mbackoff\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Max retries exceeded for chat_completion\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Max retries exceeded for chat_completion"
          ]
        }
      ],
      "source": [
        "def summarize_conversation(conversation_id: str, messages: List[Dict[str, str]], client: GroqClientWrapper, max_tokens: int = 150):\n",
        "    prompt = [SUMMARIZATION_SYSTEM_PROMPT] + format_messages_for_api(messages)\n",
        "    start = time.time()\n",
        "    summary = client.chat_completion(prompt, max_tokens=max_tokens)\n",
        "    end = time.time()\n",
        "    return {\n",
        "        \"conversation_id\": conversation_id,\n",
        "        \"summary\": summary,\n",
        "        \"metadata\": {\"runtime_sec\": round(end - start, 2)}\n",
        "    }\n",
        "\n",
        "# Test\n",
        "df = load_sample_conversations()\n",
        "client = GroqClientWrapper(GROQ_API_KEY)\n",
        "example = summarize_conversation(df.iloc[0].conversation_id, df.iloc[0].messages, client)\n",
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "rS_KKrnUUv8V",
        "outputId": "d5f81578-1101-4a51-ae4b-6be8baae1716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:GroqClientWrapper:Error: Error code: 404 - {'error': {'message': 'The model `gpt-3` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}, retrying in 1s...\n",
            "WARNING:GroqClientWrapper:Error: Error code: 404 - {'error': {'message': 'The model `gpt-3` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}, retrying in 2s...\n",
            "WARNING:GroqClientWrapper:Error: Error code: 404 - {'error': {'message': 'The model `gpt-3` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}, retrying in 4s...\n",
            "WARNING:GroqClientWrapper:Error: Error code: 404 - {'error': {'message': 'The model `gpt-3` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}, retrying in 8s...\n",
            "WARNING:GroqClientWrapper:Error: Error code: 404 - {'error': {'message': 'The model `gpt-3` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}, retrying in 16s...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Max retries exceeded for chat_completion",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2634704970.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Quick test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_structured\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconversation_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2634704970.py\u001b[0m in \u001b[0;36mextract_structured\u001b[0;34m(conversation_id, messages, client, max_tokens)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_structured\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversation_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGroqClientWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSTRUCTURED_EXTRACTION_SYSTEM_PROMPT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mformat_messages_for_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_parse_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1127568219.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, temperature, max_tokens, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mbackoff\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Max retries exceeded for chat_completion\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Max retries exceeded for chat_completion"
          ]
        }
      ],
      "source": [
        "def extract_structured(conversation_id: str, messages: List[Dict[str, str]], client: GroqClientWrapper, max_tokens: int = 512):\n",
        "    prompt = [STRUCTURED_EXTRACTION_SYSTEM_PROMPT] + format_messages_for_api(messages)\n",
        "    response = client.chat_completion(prompt, max_tokens=max_tokens)\n",
        "\n",
        "    data, valid_json = safe_parse_json(response)\n",
        "    if not valid_json or not isinstance(data, dict):\n",
        "        return {\"conversation_id\": conversation_id, \"valid_json\": False}, False\n",
        "\n",
        "    # Ensure required keys exist\n",
        "    required_keys = [\"conversation_id\",\"summary\",\"intent\",\"topics\",\"sentiment\",\"named_entities\",\"action_items\",\"confidence\",\"schema_version\"]\n",
        "    for key in required_keys:\n",
        "        if key not in data:\n",
        "            data[key] = \"\" if key not in [\"topics\",\"named_entities\",\"action_items\"] else []\n",
        "    try:\n",
        "        data[\"confidence\"] = min(max(float(data.get(\"confidence\", 0.0)), 0.0), 1.0)\n",
        "    except:\n",
        "        data[\"confidence\"] = 0.0\n",
        "\n",
        "    return data, True\n",
        "\n",
        "# Quick test\n",
        "res, ok = extract_structured(df.iloc[0].conversation_id, df.iloc[0].messages, client)\n",
        "res, ok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_NbR8hFUw4i"
      },
      "outputs": [],
      "source": [
        "def process_all(df: pd.DataFrame, client: GroqClientWrapper, out_filename=\"results.jsonl\"):\n",
        "    results = []\n",
        "    with open(out_filename, \"w\") as f:\n",
        "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "            summary_info = summarize_conversation(row.conversation_id, row.messages, client)\n",
        "            structured_info, valid = extract_structured(row.conversation_id, row.messages, client)\n",
        "            out_record = {\"conversation_id\": row.conversation_id,\n",
        "                          \"summary\": summary_info[\"summary\"],\n",
        "                          \"structured\": structured_info,\n",
        "                          \"valid_json\": valid}\n",
        "            results.append(out_record)\n",
        "            f.write(json.dumps(out_record)+\"\\n\")\n",
        "    return results\n",
        "\n",
        "# Process on sample data\n",
        "all_results = process_all(df, client)\n",
        "all_results[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65jfshQ3U4ap"
      },
      "outputs": [],
      "source": [
        "# Evaluation & Simple Tests\n",
        "\n",
        "# Test safe_parse_json\n",
        "assert safe_parse_json('{\"a\":1}')[1] == True\n",
        "assert isinstance(safe_parse_json('text {\"a\":1} text')[0], dict)\n",
        "\n",
        "# Validate extraction has required keys\n",
        "structured, ok = extract_structured(df.iloc[1].conversation_id, df.iloc[1].messages, client)\n",
        "required = [\"conversation_id\",\"summary\",\"intent\",\"topics\",\"sentiment\",\"named_entities\",\"action_items\",\"confidence\",\"schema_version\"]\n",
        "assert all(k in structured for k in required)\n",
        "\n",
        "print(\"All tests passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2XCnbAOVJVB"
      },
      "source": [
        "#Error Handling and Rate-Limits\n",
        "The **GroqClientWrapper** implements exponential backoff:\n",
        "\n",
        "* Retries up to 5 times.\n",
        "\n",
        "* On **429 Too Many Requests**, waits (1s, 2s, 4s, …).\n",
        "\n",
        "* Logs warnings but hides API keys.\n",
        "\n",
        "You can tune **retries** by modifying **retries** and **backoff** logic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhgneFtNVJ3Q"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrXpxwlxU2ZmpjmZd+WLNU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}